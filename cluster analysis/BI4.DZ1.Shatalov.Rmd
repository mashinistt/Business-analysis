---
title: "Кластерный анализ. Азия."
author: "Владимир Шаталов"
date: "29 сентября 2016 г."
output: html_document
---

В данной работе проведен кластерный анализ стран Азии по экономическим показателям.
Данные для анализа были взяты с сайта [TRADINGECONOMICS](http://www.tradingeconomics.com).

```{r}
knitr::opts_chunk$set(fih.height=30,fig.width=10,fig.align="center")

```
Загрузим наши данные и библиотеки, которыми будем пользоваться при работе, предварительно их скачав.
```{r,warning=FALSE,message=FALSE}
library(ggplot2)
library(gridExtra)
library(dendextend)
library(dplyr)
df<-read.table("D:\\AsiaClust.csv",header=T,sep=";",dec=",")
```

Пару слов о том как выбирались показатели для анализа.

* Во-первых, совокупность параметров должна адекватно отражать экономическое состояние страны.

* Во-вторых, желательно отсутствие пропущенных значений,так как во многих столбцах пропущено слишком много значений и заменить их средним или медианой, на мой взгляд неправильное решение: логичнее попробовать их предсказать по имеющимся данным, либо взять из других источников.

Посмотрим, на данные.
```{r}
knitr::kable(df,digits=2)
```

```{r}
df1<-df
df1$GDP.Pop<-df1$GDP/df1$Population
names(df1)[7]<-"GDP/Pop"
df1<-df1[,-6]
df2<-df1
df2[,2:6]<-scale(df2[,2:6])
```
Представленный выше код нормирует переменные, а также заменяет столбец с численностью населения на столбец с ВВП на душу населения.
Посмотрим на то, что получилось:
```{r}
knitr::kable(df2[1:10,],digits=2)
```

Можем приступить к кластерному анализу.

## Иерархическая кластеризация
Проведем кластеризацию трижды, по-разному вычисляя расстояния между кластерами.
```{r}
hclustWard <- hclust(dist(df2[,2:6]), method="ward.D")
hclustComplete<- hclust(dist(df2[,2:6]), method="complete")
hclustSingle<- hclust(dist(df2[,2:6]), method="single")
```
Посмотрим на дендрограммы
```{r}
plot(hclustWard, labels=df2[ ,1],hang=-1,main="Ward")
plot(hclustComplete,labels=df2[ ,1],hang=-1,main="Complete")
plot(hclustSingle,labels=df2[ ,1],hang=-1,main="Single")
```
Как видно метод Single не подходит в данном случае для кластеризации.
Построим каменистую осыпь для метода Варда
```{r}
qplot(1:(length(df2[ ,1])-1),hclustWard$height,geom=c("line","point"))
```
В зависимости от того, насколько детальные требуются результаты страны можно разбить на 5 или 9 кластеров.
Посмотрим, как в этом случае будет выглядеть дендрограмма
```{r}
plot(hclustWard, labels=df2[ ,1])
rect.hclust(hclustWard, k=5, border="tomato")
rect.hclust(hclustWard, k=9, border="green")
```
Сделаем то же самое для другого метода.
```{r}
qplot(1:(length(df2[ ,1])-1),hclustComplete$height,geom=c("line","point"))
plot(hclustComplete,labels=df2[ ,1],hang=-1,main="Complete")
rect.hclust(hclustComplete, k=9, border="tomato")
```
Здесь можно выделить 9 кластеров.
<br>
Рассмотрим какие кластеры получились, когда применялся метод Варда.
Но предварительно расчитаем средние значения всех показателей и медианы.
```{r}
means<-apply(df1[,2:6],2,mean)
medians<-apply(df1[,2:6],2,median)
means
medians
```

### Кластер 1
```{r}
groupsW5 <- cutree(hclustWard, k=5) 
colMeans(df1[groupsW5==1, 2:6])
barplot(colMeans(df2[groupsW5==1, 2:6]),main="Кластер 1")
df1[groupsW5==1,1]
```
Как видим, Китай стоит особняком за счеет своего огромного ВВП, более чем в 2 раза превышающего показатель ближайшего конкурента, в 20 раз превышающего среднее значение и в 150 раз большего медианы. При этом уровень ВВП на душу на селения довольно средний для региона.

### Кластер 2
```{r}
colMeans(df1[groupsW5==2, 2:6])
barplot(colMeans(df2[groupsW5==2, 2:6]),main="Кластер 2")
df1[groupsW5==2,1]
```
В данный кластер попали экономически развитые страны с самым высоким уровнем ВВП на душу населения, и низкими инфляцией и уровнем безработицы в регионе.

### Кластер 3
```{r}
colMeans(df1[groupsW5==3, 2:6])
barplot(colMeans(df2[groupsW5==3, 2:6]),main="Кластер 3")
df1[groupsW5==3,1]
```

В этот кластер попало довольно много стран и средние показатели по кластеру многого о них не скажут. Но при этом почти у всех у них невысокий показатель ВВП на душу населения. Но при этом намечен рост ВВП.Хотя сам ВВП небольшой. Если не учитываться вклад Индии, то средний ВВП будет равен `r round((sum(df1[groupsW5==3,2])-2074)/22,digits=2)`. Кластеризация до 9 кластеров как раз и поможет уточнить данные по этим странам.

### Кластер 4
```{r}
colMeans(df1[groupsW5==4, 2:6])
barplot(colMeans(df2[groupsW5==4, 2:6]),main="Кластер 4")
df1[groupsW5==4,1]
```
В кластер попали страны с явными проблемами на рынке труда. Если в предыдущем кластере уровень безработицы был ниже среднего, то здесь наоборот заметно выше.

### Кластер 5
```{r}
colMeans(df1[groupsW5==5, 2:6])
barplot(colMeans(df2[groupsW5==5, 2:6]),main="Кластер 5")
df1[groupsW5==5,1]
```

<p>В эту группу попали страны с самыми серьезными экономическими проблемами в Азии,о чем можно судить прежде всего по огромному снижению ВВП, большой инфляцией и высоим уровнем безработицы. В первую очередь это обусловлено вооруженными конфликтами в этих странах. К тому же в Йемене большую часть экономики составляет нефтедобывающий сектор. Мало того, что цена на нефть заметно упала, так еще и запасы нефти в стране сильно сокращаются.</p>

<p>Дальше сравним какие кластеры получились в методах Варда и Complete, когда мы делим страны на 9 кластеров.Кластеры, которые совпадают с уже рассмотренными, будем пропускать.</p>

```{r}
groupsC9 <- cutree(hclustComplete, k=9) 
groupsW9 <- cutree(hclustWard, k=9) 
```

```{r}
df1[groupsC9==3,1]
df1[groupsW9==3,1]
colMeans(df1[groupsC9==3, 2:6])
colMeans(df1[groupsW9==3, 2:6])
barplot(colMeans(df2[groupsC9==3, 2:6]),main="Кластер C3")
barplot(colMeans(df2[groupsW9==3, 2:6]),main="Кластер W3")
colMeans(df1[groupsW5==3, 2:6])

```
<br>
Два метода между собой дают не очень отличающиеся результаты.При этом в новый третий кластер из старого выбраны страны с большим производством, если судить по ВВП и его приросту, но меньшим ВВП на душу населения. Метод Варда выглядит более предпочтительно.
<br>
```{r}
df1[groupsC9==7,1]
df1[groupsW9==4,1]
colMeans(df1[groupsC9==7, 2:6])
colMeans(df1[groupsW9==4, 2:6])
barplot(colMeans(df2[groupsC9==5, 2:6]),main="Кластер C7")
barplot(colMeans(df2[groupsW9==4, 2:6]),main="Кластер W4")
colMeans(df1[groupsW5==3, 2:6])
```
Метод Варда выбрал экономически менее развитые страны из старого третьего кластера. Метод Complete выбрал один Оман.
```{r}
df1[groupsC9==9,1]
df1[groupsW9==9,1]
colMeans(df1[groupsC9==9, 2:6])
colMeans(df1[groupsW9==9, 2:6])
barplot(colMeans(df2[groupsC9==9, 2:6]),main="Кластер C9")
barplot(colMeans(df2[groupsW9==9, 2:6]),main="Кластер W9")
```
Оба метода выделили в новый кластер Афганистан и Палестину из-за огромного уровня безработицы, при прочих ничем не выделяющихся показателей.
Также оба метода выделили Макао и Катар из-за чрезвычайно высоких ВВП на душу населения.

### Вывод для иерархической кластеризации

Метод Варда показал себя лучше остальных. В итоге, можно выделить как 5, так и 9 кластеров, в зависимости от глубины рассмотрения вопроса.
Итоговые дендрограммы выглядят следующим образом.
```{r}
row.names(df2)<-df2[,1]
dendh6<-as.dendrogram(hclust(dist(df2[,2:6]), method="ward.D"))
dendh6%>%set("labels_colors",k=6)%>%set("branches_k_col",k=6)%>%set("branches_lwd",2)%>%plot(main="Дедрограмма для 6 кластеров")

dendh9<-as.dendrogram(hclust(dist(df2[,2:6]), method="ward.D"))
dendh9%>%set("labels_colors",k=9)%>%set("branches_k_col",k=9)%>%set("branches_lwd",2)%>%plot(main="Дедрограмма для 9 кластеров")
```

## Kmeans

Для начала построим каменистую осыпь, чтобы определиться с количеством кластеров.
```{r}
TW<-rep(0,15)
set.seed(20)
for (i in 1:15)
{
  TW[i]<-kmeans(df2[,2:6],centers=i)$tot.withinss
}
qplot(1:15, TW, xlab="Number of Clusters", ylab="Within groups sum of squares",geom=c("point","line"))

```

Видим, что есть смысл выделить 5 или 7 кластеров.

### Kmeans. 5 кластеров.
```{r}
set.seed(1234)
kmean5<-kmeans(df2[,2:6], 5, iter.max = 100,nstart = 50)
kmean5$centers
for (i in 1:5){print(kmean5$cluster[kmean5$cluster==i])}
```
Kmeans выбрал один большой кластер со странанами, которые иерархическая кластеризация разбила на два, выделив страны с большим уровнем безработицы. При этом отдельно выделил Сирию и объединил Йемен и Афганистан, в то время как до этого Йемен и Сирия были в одном кластере, а Афганистан в кластере со странами с высоким уровнем безработицы.
```{r}
df4<-df1[,2:6]
row.names(df4)<-df[,1]
df4[c("Afghanistan","Syria","Yemen"),]
```
Одназначно сказать тяжело, так как оба варианта имеют права на жизнь, но на мой взгляд Йемен все-таки ближе к Афганистану.
### Kmeans. 7 кластеров.
Посмотрим, что будет, если разделить страны на 7 кластеров.
```{r}
set.seed(1234)
kmean7<-kmeans(df2[,2:6], 7, iter.max = 100,nstart = 50)
kmean7$centers
for (i in 1:7){print(kmean7$cluster[kmean7$cluster==i])}
```
Получилось довольно хорошее разбиение на кластеры. Сразу стоит сказать, что порядок кластеров можем поменяться, но их состав должен остаться прежним.

* Кластер 1. Отдельно выделен Йемен из-за огромного падения ВВП.

* Кластер 2. Страны с очень высоким уровнем безработицы и довольно слабой экономикой, в которых однако замечен рост ВВП.

* Кластер 3. Китай со своим громадным ВВП.

* Кластер 4. Самые развитые страны региона.

* Кластер 5. В него входят страны со слабой экономикой, для которых характерен небольшой ВВП и выскоий уровень безработицы. Однако инфляция в таких странах мала, по сравнению с остальными странами из региона. Этот кластер получился путем выделения стран с наиболее слабой экономикой из самого большего кластера предудущей кластеризации.

* Кластер 6. Сюда попала Сирия из-за плохого экономического положения, в первую очередь проявляющегося в очень высокой инфляции.

* Кластер 7. Страны, которые можно назвать середняками азиатского региона. Хотя, конечно, экономическое положение которых может заметно отличаться.

<br>
<p> Попробуем визуализировать полученные результаты на плоскости</p>
```{r}
df2.dist=dist(df2[,2:6])
df2.mds=cmdscale(df2.dist)
dfplot<-as.data.frame(df2.mds)
dfplot$cl<-as.factor(kmean7$cluster)
xc<-rep(0,7)
for (i in 1:7){
  xc[i]<-sum(dfplot[which(dfplot[,3]==i),1])/length(dfplot[which(dfplot[,3]==i),1])
}
yc<-rep(0,7)
for (i in 1:7){
  yc[i]<-sum(dfplot[which(dfplot[,3]==i),2])/length(dfplot[which(dfplot[,3]==i),2])
}
centers<-data.frame(xc=xc,yc=yc)
ggplot(dfplot,aes(x=V1,y=V2))+geom_point(aes(col=cl),size=3)+geom_point(data=centers,aes(x=xc,y=yc),col="#f9e526", size=52, alpha=.3)+
  ggtitle("7 кластеров")+theme(plot.title = element_text(lineheight=1, face="bold"))

dfplot$cl2<-as.factor(groupsW9)
xc2<-rep(0,9)
for (i in 1:9){
  xc2[i]<-sum(dfplot[which(dfplot[,4]==i),1])/length(dfplot[which(dfplot[,4]==i),1])
}
yc2<-rep(0,9)
for (i in 1:9){
  yc2[i]<-sum(dfplot[which(dfplot[,4]==i),2])/length(dfplot[which(dfplot[,4]==i),2])
}
centers2<-data.frame(xc=xc2,yc=yc2)
ggplot(dfplot,aes(x=V1,y=V2))+geom_point(aes(col=cl2),size=3)+geom_point(data=centers2,aes(x=xc,y=yc),col="#f9e526", size=52, alpha=.3)+
  ggtitle("9 кластеров")+theme(plot.title = element_text(lineheight=1, face="bold"))

```

На основе этих графиков можно сделать лишь поверхностные выводы, так как довольно большая часть информации потеряна в связи с переходом к двум осям. Например, если бы пытались сократить размерность методом главных компонент, то сохранили бы чуть более половины всей информации.
```{r}
dfpca<-prcomp(df2[,2:6])
summary(dfpca)
```
Однако, определенное представление о данных получить можно. В частности хорошо заметны сильно выделяющиеся страны.

### Вывод

Методы иерархической кластеризации показали себя довольно неплохо и дали схожие результаты. Число выделяемых кластеров может варьироваться в зависимости от того, насколько детальный анализ необходим. Для более точной кластеризации также можно попробовать взять больше экономических показателей или попробовать другие методы кластеризации.На мой взгляд 7 кластеров, полученные в результате использования kmeans, являются в нашем случае золотой серединой между 5 и 9 кластерами и поддаются неплохой интерпретации. Стоит также заметить, что каменистая осыпь для kmeans давала разные результаты в зависимости от начального выбора центроидов. Так, на различных графиках можно было выделить 4,5,6,7 и 9 кластеров. Поэтому при выборе числа кластеров имеет смысл также ориентироваться на иерархическую кластеризацию, так как она показала неплохие результаты.


